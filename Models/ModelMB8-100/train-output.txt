2023-01-17 10:43:27.051651: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-17 10:43:27.487917: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-01-17 10:43:27.487952: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-01-17 10:43:27.487957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Number of files: 584984
Shuffled files.
2023-01-17 10:43:29.004716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-17 10:43:29.021154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-17 10:43:29.021292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-17 10:43:29.021597: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-17 10:43:29.021942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-17 10:43:29.022055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-17 10:43:29.022150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-17 10:43:29.287774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-17 10:43:29.287908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-17 10:43:29.288009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-17 10:43:29.288090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5719 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:07:00.0, compute capability: 8.6
WARNING:tensorflow:From /home/ilmari/python/moska/moska_env/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.
Instructions for updating:
Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089
(array([14.,  6.,  6.,  7.,  6.,  0.,  0.,  0., 27.,  1.,  1.,  1., 28.,
       -1., -1.,  2., 29.,  2.,  2.,  3., 30., -1.,  3.,  4., 31., -1.,
       -1.,  5., 32., -1.,  4.,  6., 33.,  3., -1., -1., 34.,  4.,  5.,
        7., 35.,  5., -1., -1., 36.,  6.,  6.,  8., 37.,  7.,  7., -1.,
       38.,  8.,  8., -1., 39.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  6.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  2.,  1.,  1.,  1.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 29.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0., 36.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
       39.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,  0.,  0.,
        0.,  0., 33.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  7.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,
        2.,  0.,  0.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        4.,  0.,  0.,  0.,  0.,  0.,  0.,  4.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.], dtype=float32), 0.0)
2023-01-17 10:43:29.586867: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.
2023-01-17 10:43:29.586885: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.
2023-01-17 10:43:29.587860: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1664] Profiler found 1 GPUs
2023-01-17 10:43:29.588009: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory
2023-01-17 10:43:29.696319: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:192] cuptiSubscribe: error 35: CUPTI_ERROR_INSUFFICIENT_PRIVILEGES
2023-01-17 10:43:29.696346: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:457] cuptiGetResultString: ignored due to a previous error.
2023-01-17 10:43:29.696355: E tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1715] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error 
2023-01-17 10:43:29.696403: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.
2023-01-17 10:43:29.696445: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:140] cuptiFinalize: ignored due to a previous error.
2023-01-17 10:43:29.696454: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:457] cuptiGetResultString: ignored due to a previous error.
2023-01-17 10:43:29.696461: E tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1807] function cupti_interface_->Finalize()failed with error 
Epoch 1/100
2023-01-17 10:43:34.643096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2023-01-17 10:43:34.661655: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x638ec90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-01-17 10:43:34.661675: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Ti, Compute Capability 8.6
2023-01-17 10:43:34.678182: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-01-17 10:43:34.870978: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2023-01-17 10:43:34.871906: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6
2023-01-17 10:43:34.871921: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:237] Used ptxas at ptxas
2023-01-17 10:43:34.872864: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-01-17 10:43:34.914948: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2023-01-17 10:43:34.957480: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2023-01-17 10:43:35.039998: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2023-01-17 10:43:35.083350: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2023-01-17 10:43:35.125599: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2023-01-17 10:43:35.167339: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2023-01-17 10:43:35.209452: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2023-01-17 10:43:35.250717: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
      1/Unknown - 6s 6s/step - loss: 0.5305 - accuracy: 0.70312023-01-17 10:43:35.271930: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.
2023-01-17 10:43:35.271948: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.
2023-01-17 10:43:35.271970: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:133] cuptiGetTimestamp: ignored due to a previous error.
2023-01-17 10:43:35.271975: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:184] cuptiSubscribe: ignored due to a previous error.
2023-01-17 10:43:35.271979: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:457] cuptiGetResultString: ignored due to a previous error.
2023-01-17 10:43:35.271982: E tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1715] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error 
2023-01-17 10:43:35.278762: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.
2023-01-17 10:43:35.280119: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:140] cuptiFinalize: ignored due to a previous error.
2023-01-17 10:43:35.280132: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:457] cuptiGetResultString: ignored due to a previous error.
2023-01-17 10:43:35.280138: E tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1807] function cupti_interface_->Finalize()failed with error 
2023-01-17 10:43:35.293154: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:133] cuptiGetTimestamp: ignored due to a previous error.
2023-01-17 10:43:35.293179: I tensorflow/core/profiler/backends/gpu/cupti_collector.cc:522]  GpuTracer has collected 0 callback api events and 0 activity events. 
2023-01-17 10:43:35.293901: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.
2023-01-17 10:43:35.294771: I tensorflow/core/profiler/rpc/client/save_profile.cc:164] Collecting XSpace to repository: tensorboard-log/plugins/profile/2023_01_17_10_43_35/ilmari99.xplane.pb
5351/5351 [==============================] - 305s 56ms/step - loss: 0.5307 - accuracy: 0.7076 - val_loss: 0.5235 - val_accuracy: 0.7154
Epoch 2/100
5351/5351 [==============================] - 319s 59ms/step - loss: 0.5293 - accuracy: 0.7086 - val_loss: 0.5240 - val_accuracy: 0.7157
Epoch 3/100
5351/5351 [==============================] - 342s 63ms/step - loss: 0.5283 - accuracy: 0.7094 - val_loss: 0.5230 - val_accuracy: 0.7168
Epoch 4/100
5351/5351 [==============================] - 349s 64ms/step - loss: 0.5275 - accuracy: 0.7100 - val_loss: 0.5225 - val_accuracy: 0.7170
Epoch 5/100
5351/5351 [==============================] - 346s 64ms/step - loss: 0.5267 - accuracy: 0.7107 - val_loss: 0.5224 - val_accuracy: 0.7189
Epoch 6/100
5351/5351 [==============================] - 345s 64ms/step - loss: 0.5261 - accuracy: 0.7111 - val_loss: 0.5224 - val_accuracy: 0.7181
Epoch 7/100
5351/5351 [==============================] - 348s 64ms/step - loss: 0.5254 - accuracy: 0.7115 - val_loss: 0.5224 - val_accuracy: 0.7178
Epoch 8/100
5351/5351 [==============================] - 346s 64ms/step - loss: 0.5248 - accuracy: 0.7121 - val_loss: 0.5216 - val_accuracy: 0.7189
Epoch 9/100
5351/5351 [==============================] - 345s 64ms/step - loss: 0.5243 - accuracy: 0.7124 - val_loss: 0.5215 - val_accuracy: 0.7196
Epoch 10/100
5351/5351 [==============================] - 346s 64ms/step - loss: 0.5238 - accuracy: 0.7128 - val_loss: 0.5209 - val_accuracy: 0.7202
Epoch 11/100
5351/5351 [==============================] - 345s 64ms/step - loss: 0.5234 - accuracy: 0.7131 - val_loss: 0.5203 - val_accuracy: 0.7201
Epoch 12/100
5351/5351 [==============================] - 343s 63ms/step - loss: 0.5229 - accuracy: 0.7134 - val_loss: 0.5202 - val_accuracy: 0.7196
Epoch 13/100
5351/5351 [==============================] - 341s 63ms/step - loss: 0.5225 - accuracy: 0.7139 - val_loss: 0.5205 - val_accuracy: 0.7201
Epoch 14/100
5351/5351 [==============================] - 341s 63ms/step - loss: 0.5221 - accuracy: 0.7141 - val_loss: 0.5197 - val_accuracy: 0.7206
Epoch 15/100
5351/5351 [==============================] - 341s 63ms/step - loss: 0.5217 - accuracy: 0.7143 - val_loss: 0.5196 - val_accuracy: 0.7211
Epoch 16/100
5351/5351 [==============================] - 343s 63ms/step - loss: 0.5214 - accuracy: 0.7146 - val_loss: 0.5202 - val_accuracy: 0.7207
Epoch 17/100
5351/5351 [==============================] - 344s 63ms/step - loss: 0.5210 - accuracy: 0.7149 - val_loss: 0.5199 - val_accuracy: 0.7210
Epoch 18/100
5351/5351 [==============================] - 344s 64ms/step - loss: 0.5207 - accuracy: 0.7152 - val_loss: 0.5196 - val_accuracy: 0.7209
Epoch 19/100
5351/5351 [==============================] - 344s 64ms/step - loss: 0.5204 - accuracy: 0.7154 - val_loss: 0.5193 - val_accuracy: 0.7220
Epoch 20/100
5351/5351 [==============================] - 344s 63ms/step - loss: 0.5201 - accuracy: 0.7156 - val_loss: 0.5191 - val_accuracy: 0.7211
Epoch 21/100
5351/5351 [==============================] - 343s 63ms/step - loss: 0.5198 - accuracy: 0.7159 - val_loss: 0.5191 - val_accuracy: 0.7215
Epoch 22/100
5351/5351 [==============================] - 343s 63ms/step - loss: 0.5195 - accuracy: 0.7159 - val_loss: 0.5181 - val_accuracy: 0.7209
Epoch 23/100
5351/5351 [==============================] - 343s 63ms/step - loss: 0.5192 - accuracy: 0.7162 - val_loss: 0.5184 - val_accuracy: 0.7215
Epoch 24/100
5351/5351 [==============================] - 343s 63ms/step - loss: 0.5190 - accuracy: 0.7165 - val_loss: 0.5180 - val_accuracy: 0.7212
Epoch 25/100
5351/5351 [==============================] - 343s 63ms/step - loss: 0.5187 - accuracy: 0.7166 - val_loss: 0.5182 - val_accuracy: 0.7219
Epoch 26/100
5351/5351 [==============================] - 343s 63ms/step - loss: 0.5185 - accuracy: 0.7167 - val_loss: 0.5178 - val_accuracy: 0.7224
Epoch 27/100
5351/5351 [==============================] - 343s 63ms/step - loss: 0.5183 - accuracy: 0.7169 - val_loss: 0.5176 - val_accuracy: 0.7232
Epoch 28/100
5351/5351 [==============================] - 343s 63ms/step - loss: 0.5180 - accuracy: 0.7171 - val_loss: 0.5173 - val_accuracy: 0.7226
Epoch 29/100
5351/5351 [==============================] - 343s 63ms/step - loss: 0.5179 - accuracy: 0.7172 - val_loss: 0.5172 - val_accuracy: 0.7230
Epoch 30/100
5351/5351 [==============================] - 344s 64ms/step - loss: 0.5176 - accuracy: 0.7175 - val_loss: 0.5169 - val_accuracy: 0.7233
Epoch 31/100
5351/5351 [==============================] - 345s 64ms/step - loss: 0.5174 - accuracy: 0.7176 - val_loss: 0.5171 - val_accuracy: 0.7230
Epoch 32/100
5351/5351 [==============================] - 344s 64ms/step - loss: 0.5173 - accuracy: 0.7178 - val_loss: 0.5169 - val_accuracy: 0.7234
Epoch 33/100
5351/5351 [==============================] - 345s 64ms/step - loss: 0.5170 - accuracy: 0.7179 - val_loss: 0.5168 - val_accuracy: 0.7233
Epoch 34/100
5351/5351 [==============================] - 344s 64ms/step - loss: 0.5168 - accuracy: 0.7181 - val_loss: 0.5166 - val_accuracy: 0.7238
Epoch 35/100
5351/5351 [==============================] - 345s 64ms/step - loss: 0.5167 - accuracy: 0.7182 - val_loss: 0.5161 - val_accuracy: 0.7237
Epoch 36/100
5351/5351 [==============================] - 344s 64ms/step - loss: 0.5166 - accuracy: 0.7182 - val_loss: 0.5161 - val_accuracy: 0.7238
Epoch 37/100
5351/5351 [==============================] - 344s 64ms/step - loss: 0.5163 - accuracy: 0.7185 - val_loss: 0.5164 - val_accuracy: 0.7229
Epoch 38/100
5351/5351 [==============================] - 345s 64ms/step - loss: 0.5161 - accuracy: 0.7186 - val_loss: 0.5157 - val_accuracy: 0.7240
Epoch 39/100
5351/5351 [==============================] - 347s 64ms/step - loss: 0.5160 - accuracy: 0.7187 - val_loss: 0.5157 - val_accuracy: 0.7239
Epoch 40/100
5351/5351 [==============================] - 346s 64ms/step - loss: 0.5158 - accuracy: 0.7187 - val_loss: 0.5155 - val_accuracy: 0.7240
Epoch 41/100
5351/5351 [==============================] - 346s 64ms/step - loss: 0.5157 - accuracy: 0.7190 - val_loss: 0.5163 - val_accuracy: 0.7235
Epoch 42/100
5351/5351 [==============================] - 349s 65ms/step - loss: 0.5156 - accuracy: 0.7190 - val_loss: 0.5159 - val_accuracy: 0.7236
Epoch 43/100
5351/5351 [==============================] - 353s 65ms/step - loss: 0.5154 - accuracy: 0.7192 - val_loss: 0.5154 - val_accuracy: 0.7244
Epoch 44/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5152 - accuracy: 0.7192 - val_loss: 0.5150 - val_accuracy: 0.7242
Epoch 45/100
5351/5351 [==============================] - 345s 64ms/step - loss: 0.5151 - accuracy: 0.7193 - val_loss: 0.5155 - val_accuracy: 0.7239
Epoch 46/100
5351/5351 [==============================] - 346s 64ms/step - loss: 0.5150 - accuracy: 0.7196 - val_loss: 0.5158 - val_accuracy: 0.7233
Epoch 47/100
5351/5351 [==============================] - 347s 64ms/step - loss: 0.5148 - accuracy: 0.7197 - val_loss: 0.5157 - val_accuracy: 0.7247
Epoch 48/100
5351/5351 [==============================] - 352s 65ms/step - loss: 0.5147 - accuracy: 0.7197 - val_loss: 0.5153 - val_accuracy: 0.7249
Epoch 49/100
5351/5351 [==============================] - 352s 65ms/step - loss: 0.5146 - accuracy: 0.7198 - val_loss: 0.5150 - val_accuracy: 0.7241
Epoch 50/100
5351/5351 [==============================] - 349s 64ms/step - loss: 0.5144 - accuracy: 0.7199 - val_loss: 0.5147 - val_accuracy: 0.7250
Epoch 51/100
5351/5351 [==============================] - 353s 65ms/step - loss: 0.5143 - accuracy: 0.7200 - val_loss: 0.5144 - val_accuracy: 0.7246
Epoch 52/100
5351/5351 [==============================] - 349s 64ms/step - loss: 0.5143 - accuracy: 0.7200 - val_loss: 0.5143 - val_accuracy: 0.7249
Epoch 53/100
5351/5351 [==============================] - 348s 64ms/step - loss: 0.5141 - accuracy: 0.7202 - val_loss: 0.5145 - val_accuracy: 0.7253
Epoch 54/100
5351/5351 [==============================] - 349s 64ms/step - loss: 0.5140 - accuracy: 0.7202 - val_loss: 0.5146 - val_accuracy: 0.7239
Epoch 55/100
5351/5351 [==============================] - 349s 65ms/step - loss: 0.5139 - accuracy: 0.7203 - val_loss: 0.5145 - val_accuracy: 0.7246
Epoch 56/100
5351/5351 [==============================] - 357s 66ms/step - loss: 0.5137 - accuracy: 0.7203 - val_loss: 0.5144 - val_accuracy: 0.7242
Epoch 57/100
5351/5351 [==============================] - 352s 65ms/step - loss: 0.5137 - accuracy: 0.7205 - val_loss: 0.5143 - val_accuracy: 0.7246
Epoch 58/100
5351/5351 [==============================] - 349s 65ms/step - loss: 0.5135 - accuracy: 0.7205 - val_loss: 0.5139 - val_accuracy: 0.7248
Epoch 59/100
5351/5351 [==============================] - 350s 65ms/step - loss: 0.5134 - accuracy: 0.7207 - val_loss: 0.5145 - val_accuracy: 0.7253
Epoch 60/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5133 - accuracy: 0.7206 - val_loss: 0.5136 - val_accuracy: 0.7253
Epoch 61/100
5351/5351 [==============================] - 350s 65ms/step - loss: 0.5132 - accuracy: 0.7207 - val_loss: 0.5141 - val_accuracy: 0.7253
Epoch 62/100
5351/5351 [==============================] - 356s 66ms/step - loss: 0.5132 - accuracy: 0.7209 - val_loss: 0.5137 - val_accuracy: 0.7252
Epoch 63/100
5351/5351 [==============================] - 349s 65ms/step - loss: 0.5130 - accuracy: 0.7209 - val_loss: 0.5134 - val_accuracy: 0.7263
Epoch 64/100
5351/5351 [==============================] - 350s 65ms/step - loss: 0.5130 - accuracy: 0.7210 - val_loss: 0.5140 - val_accuracy: 0.7244
Epoch 65/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5128 - accuracy: 0.7211 - val_loss: 0.5137 - val_accuracy: 0.7261
Epoch 66/100
5351/5351 [==============================] - 350s 65ms/step - loss: 0.5127 - accuracy: 0.7212 - val_loss: 0.5139 - val_accuracy: 0.7251
Epoch 67/100
5351/5351 [==============================] - 357s 66ms/step - loss: 0.5126 - accuracy: 0.7212 - val_loss: 0.5137 - val_accuracy: 0.7252
Epoch 68/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5127 - accuracy: 0.7212 - val_loss: 0.5138 - val_accuracy: 0.7261
Epoch 69/100
5351/5351 [==============================] - 357s 66ms/step - loss: 0.5124 - accuracy: 0.7213 - val_loss: 0.5131 - val_accuracy: 0.7263
Epoch 70/100
5351/5351 [==============================] - 352s 65ms/step - loss: 0.5124 - accuracy: 0.7214 - val_loss: 0.5133 - val_accuracy: 0.7261
Epoch 71/100
5351/5351 [==============================] - 352s 65ms/step - loss: 0.5123 - accuracy: 0.7214 - val_loss: 0.5137 - val_accuracy: 0.7255
Epoch 72/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5122 - accuracy: 0.7215 - val_loss: 0.5136 - val_accuracy: 0.7265
Epoch 73/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5122 - accuracy: 0.7216 - val_loss: 0.5135 - val_accuracy: 0.7254
Epoch 74/100
5351/5351 [==============================] - 357s 66ms/step - loss: 0.5121 - accuracy: 0.7216 - val_loss: 0.5132 - val_accuracy: 0.7256
Epoch 75/100
5351/5351 [==============================] - 357s 66ms/step - loss: 0.5120 - accuracy: 0.7218 - val_loss: 0.5132 - val_accuracy: 0.7255
Epoch 76/100
5351/5351 [==============================] - 361s 67ms/step - loss: 0.5119 - accuracy: 0.7218 - val_loss: 0.5131 - val_accuracy: 0.7266
Epoch 77/100
5351/5351 [==============================] - 355s 66ms/step - loss: 0.5118 - accuracy: 0.7218 - val_loss: 0.5130 - val_accuracy: 0.7265
Epoch 78/100
5351/5351 [==============================] - 359s 66ms/step - loss: 0.5117 - accuracy: 0.7219 - val_loss: 0.5133 - val_accuracy: 0.7263
Epoch 79/100
5351/5351 [==============================] - 357s 66ms/step - loss: 0.5117 - accuracy: 0.7219 - val_loss: 0.5130 - val_accuracy: 0.7258
Epoch 80/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5116 - accuracy: 0.7219 - val_loss: 0.5131 - val_accuracy: 0.7262
Epoch 81/100
5351/5351 [==============================] - 352s 65ms/step - loss: 0.5116 - accuracy: 0.7220 - val_loss: 0.5130 - val_accuracy: 0.7270
Epoch 82/100
5351/5351 [==============================] - 352s 65ms/step - loss: 0.5115 - accuracy: 0.7220 - val_loss: 0.5132 - val_accuracy: 0.7261
Epoch 83/100
5351/5351 [==============================] - 352s 65ms/step - loss: 0.5114 - accuracy: 0.7221 - val_loss: 0.5128 - val_accuracy: 0.7256
Epoch 84/100
5351/5351 [==============================] - 357s 66ms/step - loss: 0.5114 - accuracy: 0.7221 - val_loss: 0.5132 - val_accuracy: 0.7267
Epoch 85/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5113 - accuracy: 0.7222 - val_loss: 0.5134 - val_accuracy: 0.7262
Epoch 86/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5112 - accuracy: 0.7222 - val_loss: 0.5128 - val_accuracy: 0.7269
Epoch 87/100
5351/5351 [==============================] - 356s 66ms/step - loss: 0.5112 - accuracy: 0.7222 - val_loss: 0.5129 - val_accuracy: 0.7267
Epoch 88/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5111 - accuracy: 0.7224 - val_loss: 0.5131 - val_accuracy: 0.7269
Epoch 89/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5111 - accuracy: 0.7223 - val_loss: 0.5133 - val_accuracy: 0.7268
Epoch 90/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5110 - accuracy: 0.7224 - val_loss: 0.5123 - val_accuracy: 0.7267
Epoch 91/100
5351/5351 [==============================] - 356s 66ms/step - loss: 0.5109 - accuracy: 0.7225 - val_loss: 0.5129 - val_accuracy: 0.7271
Epoch 92/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5108 - accuracy: 0.7226 - val_loss: 0.5124 - val_accuracy: 0.7264
Epoch 93/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5108 - accuracy: 0.7225 - val_loss: 0.5128 - val_accuracy: 0.7272
Epoch 94/100
5351/5351 [==============================] - 350s 65ms/step - loss: 0.5107 - accuracy: 0.7226 - val_loss: 0.5128 - val_accuracy: 0.7274
Epoch 95/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5106 - accuracy: 0.7227 - val_loss: 0.5126 - val_accuracy: 0.7273
Epoch 96/100
5351/5351 [==============================] - 352s 65ms/step - loss: 0.5107 - accuracy: 0.7227 - val_loss: 0.5123 - val_accuracy: 0.7265
Epoch 97/100
5351/5351 [==============================] - 352s 65ms/step - loss: 0.5105 - accuracy: 0.7227 - val_loss: 0.5126 - val_accuracy: 0.7265
Epoch 98/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5105 - accuracy: 0.7228 - val_loss: 0.5125 - val_accuracy: 0.7265
Epoch 99/100
5351/5351 [==============================] - 351s 65ms/step - loss: 0.5105 - accuracy: 0.7228 - val_loss: 0.5123 - val_accuracy: 0.7262
Epoch 100/100
5351/5351 [==============================] - 355s 66ms/step - loss: 0.5104 - accuracy: 0.7229 - val_loss: 0.5121 - val_accuracy: 0.7272
25/25 [==============================] - 2s 58ms/step - loss: 0.5070 - accuracy: 0.7275